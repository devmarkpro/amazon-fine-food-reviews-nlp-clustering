{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3039a4",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this project, we will use the [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews/) dataset. It contains reviews of fine foods from Amazon, including text reviews and ratings. Here's the list of columns in the dataset:\n",
    "\n",
    "- `Id`: Unique identifier for each review\n",
    "- `ProductId`: Unique identifier for the product\n",
    "- `UserId`: Unique identifier for the user\n",
    "- `ProfileName`: Name of the user profile\n",
    "- `HelpfulnessNumerator`: Number of helpful votes\n",
    "- `HelpfulnessDenominator`: Total number of votes\n",
    "- `Score`: Rating given by the user (1 to 5)\n",
    "- `Time`: Timestamp of the review\n",
    "- `Summary`: Summary of the review\n",
    "- `Text`: Full text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33872728",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "random_state = 42\n",
    "dataset_sample_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f766b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import html\n",
    "import re\n",
    "class WordTokenizer:\n",
    "    \"\"\"\n",
    "    A class for tokenizing and preprocessing text data using NLTK.\n",
    "    This class handles text normalization, punctuation removal, and optional stop word filtering.\n",
    "\n",
    "    Attributes:\n",
    "        remove_stopwords (bool): Whether to remove stop words.\n",
    "        lower_case (bool): Whether to convert text to lowercase.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_stopwords: bool = True,\n",
    "        lower_case: bool = True,\n",
    "        use_lemmatization: bool = True,\n",
    "        replace_repeated_chars: bool = True,\n",
    "    ):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lower_case = lower_case\n",
    "        self.lematizer = nltk.WordNetLemmatizer() if use_lemmatization else None\n",
    "        self.stop_words = set(stopwords.words(\"english\")) if remove_stopwords else set()\n",
    "        self.replace_repeated_chars = replace_repeated_chars\n",
    "\n",
    "    def process(\n",
    "        self, text: Union[str, List[str]], return_tokens: bool = False\n",
    "    ) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Process the input text by cleaning, normalizing, and optionally removing stop words.\n",
    "\n",
    "        Args:\n",
    "            text (str | List[str]): The input text. If a list is provided, its elements will be joined into a single string.\n",
    "            return_tokens (bool): If True, returns a list of tokens instead of a string.\n",
    "\n",
    "        Returns:\n",
    "            str | List[str]: The processed text as a string or list of tokens.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return [] if return_tokens else \"\"\n",
    "        logger.debug(f\"Processing text: {text}\")\n",
    "\n",
    "        text_str = \" \".join(map(str, text)) if isinstance(text, list) else str(text)\n",
    "\n",
    "        text = self._clean_text(text_str)\n",
    "        logger.debug(f\"Cleaned text: {text}\")\n",
    "\n",
    "        tokens = self._tokenize(text)\n",
    "        logger.debug(f\"Tokenized text: {tokens}\")\n",
    "\n",
    "        tokens = self._remove_stopwords(tokens)\n",
    "        logger.debug(f\"Tokens after stop word removal: {tokens}\")\n",
    "\n",
    "        tokens = self._apply_lemmatization(tokens)\n",
    "        logger.debug(f\"Tokens after lemmatization: {tokens}\")\n",
    "\n",
    "        tokens = [token for token in tokens if token]  # Remove empty tokens\n",
    "\n",
    "        logger.debug(f\"Final tokens: {tokens}\")\n",
    "\n",
    "        return tokens if return_tokens else \" \".join(tokens)\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        if self.lower_case:\n",
    "            text = text.lower()\n",
    "\n",
    "        text = html.unescape(text)  # decode &amp; and other HTML entities\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        text = re.sub(r\"<[^>]*>\", \"\", text)  # Remove HTML tags\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "        text = self._fix_contractions(text)  # Fix common contractions\n",
    "\n",
    "        # reduce repeated characters like \"sooo good\" to \"so good\" or \"loooove\" to \"love\"\n",
    "        if self.replace_repeated_chars:\n",
    "            text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_lemmatization(self, tokens: List[str]) -> List[str]:\n",
    "        if self.lematizer is not None:\n",
    "            tokens = [self.lematizer.lemmatize(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def _remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "        if self.remove_stopwords:\n",
    "            tokens = self._remove_stop_words(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def _remove_stop_words(self, tokens: List[str]) -> List[str]:\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "\n",
    "    def _fix_contractions(self, text: str) -> str:\n",
    "        contraction_map = {\n",
    "            \"dont\": \"do not\",\n",
    "            \"cant\": \"can not\",\n",
    "            \"wont\": \"will not\",\n",
    "            \"isnt\": \"is not\",\n",
    "            \"arent\": \"are not\",\n",
    "            \"wasnt\": \"was not\",\n",
    "            \"werent\": \"were not\",\n",
    "            \"didnt\": \"did not\",\n",
    "            \"doesnt\": \"does not\",\n",
    "            \"havent\": \"have not\",\n",
    "            \"hasnt\": \"has not\",\n",
    "            \"hadnt\": \"had not\",\n",
    "            \"shouldnt\": \"should not\",\n",
    "            \"wouldnt\": \"would not\",\n",
    "            \"couldnt\": \"could not\",\n",
    "            \"mustnt\": \"must not\",\n",
    "            \"mightnt\": \"might not\",\n",
    "            \"neednt\": \"need not\",\n",
    "            \"im\": \"i am\",\n",
    "            \"ive\": \"i have\",\n",
    "            \"ill\": \"i will\",\n",
    "            \"id\": \"i would\",\n",
    "            \"youre\": \"you are\",\n",
    "            \"youve\": \"you have\",\n",
    "            \"youll\": \"you will\",\n",
    "            \"youd\": \"you would\",\n",
    "            \"theyre\": \"they are\",\n",
    "            \"theyve\": \"they have\",\n",
    "            \"theyll\": \"they will\",\n",
    "            \"theyd\": \"they would\",\n",
    "            \"weve\": \"we have\",\n",
    "            \"theres\": \"there is\",\n",
    "            \"heres\": \"here is\",\n",
    "            \"thats\": \"that is\",\n",
    "            \"whats\": \"what is\",\n",
    "            \"whos\": \"who is\",\n",
    "            \"don't\": \"do not\",\n",
    "            \"can't\": \"can not\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"isn't\": \"is not\",\n",
    "            \"aren't\": \"are not\",\n",
    "            \"wasn't\": \"was not\",\n",
    "            \"weren't\": \"were not\",\n",
    "            \"didn't\": \"did not\",\n",
    "            \"doesn't\": \"does not\",\n",
    "            \"haven't\": \"have not\",\n",
    "            \"hasn't\": \"has not\",\n",
    "            \"hadn't\": \"had not\",\n",
    "            \"shouldn't\": \"should not\",\n",
    "            \"wouldn't\": \"would not\",\n",
    "            \"couldn't\": \"could not\",\n",
    "            \"mustn't\": \"must not\",\n",
    "            \"mightn't\": \"might not\",\n",
    "            \"needn't\": \"need not\",\n",
    "            \"i'm\": \"i am\",\n",
    "            \"i've\": \"i have\",\n",
    "            \"i'll\": \"i will\",\n",
    "            \"i'd\": \"i would\",\n",
    "            \"you're\": \"you are\",\n",
    "            \"you've\": \"you have\",\n",
    "            \"you'll\": \"you will\",\n",
    "            \"you'd\": \"you would\",\n",
    "            \"they're\": \"they are\",\n",
    "            \"they've\": \"they have\",\n",
    "            \"they'll\": \"they will\",\n",
    "            \"they'd\": \"they would\",\n",
    "            \"we've\": \"we have\",\n",
    "        }\n",
    "        for contraction, full_form in contraction_map.items():\n",
    "            text = re.sub(r\"\\b\" + re.escape(contraction) + r\"\\b\", full_form, text)\n",
    "        return text\n",
    "\n",
    "    def __call__(\n",
    "        self, text: Union[str, List[str]], return_tokens: bool = False\n",
    "    ) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Allow the object to be called like a function.\n",
    "\n",
    "        Example:\n",
    "            tokenizer = WordTokenizer()\n",
    "            clean_text = tokenizer(\"Some input text here.\")\n",
    "        \"\"\"\n",
    "        return self.process(text, return_tokens=return_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_words_per_cluster(tfidf_matrix, labels, vectorizer, cluster, n=10):\n",
    "    indices = np.where(labels == cluster)[0]\n",
    "    # Compute mean vector for the cluster\n",
    "    cluster_tfidf = tfidf_matrix[indices].mean(axis=0)\n",
    "    # Convert to 1D array if needed\n",
    "    if hasattr(cluster_tfidf, 'A1'):\n",
    "        cluster_tfidf = cluster_tfidf.A1\n",
    "    else:\n",
    "        cluster_tfidf = np.array(cluster_tfidf).flatten()\n",
    "    # Get top n indices\n",
    "    top_n = np.argsort(cluster_tfidf)[::-1][:n]\n",
    "    res = [vectorizer.get_feature_names_out()[i] for i in top_n]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    \"\"\"\n",
    "    Custom scoring function for unsupervised clustering.\n",
    "    Fits estimator on X and returns silhouette score.\n",
    "    \"\"\"\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line if you have a pre-vectorized or pre-tokenized dataset and use it instead of the next line.\n",
    "# df = pd.read_csv(\"./data/vectorized_reviews.csv\") \n",
    "\n",
    "df = pd.read_csv(\"./data/Reviews.csv\")\n",
    "if dataset_sample_size > 0:\n",
    "    df = df.sample(n=dataset_sample_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee179030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_product = df.groupby(\"ProductId\").size().sort_values(ascending=False)\n",
    "grouped_by_user = df.groupby(\"UserId\").size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c125a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "fig.suptitle(\"FineFoodReview Dataset Analysis with Mean & Median\", fontsize=18)\n",
    "\n",
    "# 1. Histogram of reviews per user\n",
    "mean_u = grouped_by_user.mean()\n",
    "median_u = grouped_by_user.median()\n",
    "sns.histplot(grouped_by_user.to_numpy(), bins=50, ax=axes[0,0], color=\"skyblue\", kde=False)\n",
    "axes[0,0].axvline(mean_u, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_u:.2f}\")\n",
    "axes[0,0].axvline(median_u, color=\"green\", linestyle=\"-.\", label=f\"Median: {median_u}\")\n",
    "axes[0,0].set_title(\"Distribution of Number of Reviews per User\")\n",
    "axes[0,0].set_xlabel(\"Number of Reviews\")\n",
    "axes[0,0].set_ylabel(\"Number of Users\")\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Histogram of reviews per product\n",
    "mean_p = grouped_by_product.mean()\n",
    "median_p = grouped_by_product.median()\n",
    "sns.histplot(grouped_by_product.to_numpy(), bins=50, ax=axes[0,1], color=\"lightcoral\", kde=False)\n",
    "axes[0,1].axvline(mean_p, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_p:.2f}\")\n",
    "axes[0,1].axvline(median_p, color=\"green\", linestyle=\"-.\", label=f\"Median: {median_p}\")\n",
    "axes[0,1].set_title(\"Distribution of Number of Reviews per Product\")\n",
    "axes[0,1].set_xlabel(\"Number of Reviews\")\n",
    "axes[0,1].set_ylabel(\"Number of Products\")\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. CDF for users\n",
    "sns.ecdfplot(grouped_by_user.to_numpy(), ax=axes[1,0], color=\"blue\")\n",
    "axes[1,0].set_title(\"CDF of Number of Reviews per User\")\n",
    "axes[1,0].set_xlabel(\"Number of Reviews\")\n",
    "axes[1,0].set_ylabel(\"Cumulative Fraction of Users\")\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "\n",
    "# 4. CDF for products\n",
    "sns.ecdfplot(grouped_by_product.to_numpy(), ax=axes[1,1], color=\"darkorange\")\n",
    "axes[1,1].set_title(\"CDF of Number of Reviews per Product\")\n",
    "axes[1,1].set_xlabel(\"Number of Reviews\")\n",
    "axes[1,1].set_ylabel(\"Cumulative Fraction of Products\")\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "# 5. Boxplot for users\n",
    "sns.boxplot(x=grouped_by_user, ax=axes[2,0], color=\"lightblue\", showmeans=True,\n",
    "            meanprops={\"marker\":\"o\",\"markerfacecolor\":\"red\",\"markeredgecolor\":\"black\"})\n",
    "axes[2,0].set_title(\"Boxplot of Number of Reviews per User\")\n",
    "axes[2,0].set_xlabel(\"Number of Reviews\")\n",
    "\n",
    "# 6. Boxplot for products\n",
    "sns.boxplot(x=grouped_by_product, ax=axes[2,1], color=\"salmon\", showmeans=True,\n",
    "            meanprops={\"marker\":\"o\",\"markerfacecolor\":\"red\",\"markeredgecolor\":\"black\"})\n",
    "axes[2,1].set_title(\"Boxplot of Number of Reviews per Product\")\n",
    "axes[2,1].set_xlabel(\"Number of Reviews\")\n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.97))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a569c2",
   "metadata": {},
   "source": [
    "\n",
    "From the user and product statistics, we can see that the average number of reviews per user is around 2.22, with a maximum of 448 reviews by a single user. For products, the average number of reviews is about 7.66, with a maximum of 913 reviews for a single product. This indicates that while most users review only a few products, some users are very active in reviewing multiple products. Also, looking at the 75%, we can see that 75% of users have reviewed 2 or fewer products, and 75% of products have received 5 or fewer reviews. This suggests a long tail distribution where a few users and products are highly active while the majority are not. It's important to consider these statistics when training models, as they can impact the model's ability to generalize and the importance of certain users or products in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(\n",
    "            remove_stopwords=True,\n",
    "            lower_case=True,\n",
    "            use_lemmatization=True,\n",
    "        )\n",
    "def tokenize_reviews(input_df, tkz: WordTokenizer, forced = False):\n",
    "    if forced or \"TokenizedText\" not in input_df.columns:\n",
    "        input_df[\"TokenizedText\"] = input_df[\"Text\"].apply(lambda x: tkz(x, return_tokens=True))\n",
    "    return input_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenize_reviews(df, tokenizer)\n",
    "df = tokenized_df.copy(deep=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e86979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the new dataset with tokenized reviews to save time in future runs\n",
    "df.to_csv(\"data/fine_food_reviews_tokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fabcd2",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd380fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JoinedTokens'] = df['TokenizedText'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df['JoinedTokens'])\n",
    "\n",
    "df['doc_vector_tfidf'] = list(X_tfidf.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tfidf = KMeans(n_clusters=5, random_state=random_state)\n",
    "df['Cluster_TFIDF'] = kmeans_tfidf.fit_predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/vectorized_reviews.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6caf0c3",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "To optimize the KMeans clustering, we will perform hyperparameter tuning primarily on the number of clusters (k). We use the Elbow Method and Silhouette Score to determine the optimal k. The Elbow Method looks for the point where the Within-Cluster Sum of Squares (WSS) starts to diminish, indicating diminishing returns on adding more clusters. The Silhouette Score measures how similar an object is to its own cluster compared to other clusters, with higher scores indicating better-defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ade13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_dense = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310a829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KMeans pipeline params: {'cluster__n_clusters': 29, 'svd__n_components': 300}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_km = Pipeline([\n",
    "    ('svd', TruncatedSVD(random_state=random_state)),\n",
    "    ('cluster', KMeans(random_state=random_state))\n",
    "])\n",
    "\n",
    "param_grid_km = {\n",
    "    'svd__n_components': [50, 100, 200, 300],\n",
    "    'cluster__n_clusters': list(range(2, 30))\n",
    "}\n",
    "\n",
    "grid_km = GridSearchCV(\n",
    "    pipe_km,\n",
    "    param_grid_km,\n",
    "    scoring=silhouette_scorer,\n",
    "    cv=[(\n",
    "        np.arange(X_tfidf_dense.shape[0]),\n",
    "        np.arange(X_tfidf_dense.shape[0])\n",
    "    )],\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "grid_km.fit(X_tfidf_dense)\n",
    "best_params_km = grid_km.best_params_\n",
    "# Best KMeans pipeline params: {'cluster__n_clusters': 29, 'svd__n_components': 300}\n",
    "print(f\"Best KMeans pipeline params: {best_params_km}\")\n",
    "df['Cluster_KMeans_Pipe'] = grid_km.best_estimator_.named_steps['cluster'].labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf91ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Agglomerative pipeline params: {'cluster__n_clusters': 16, 'svd__n_components': 50}\n"
     ]
    }
   ],
   "source": [
    "pipe_agg = Pipeline([\n",
    "    ('svd', TruncatedSVD(random_state=random_state)),\n",
    "    ('cluster', AgglomerativeClustering(linkage='ward'))\n",
    "])\n",
    "param_grid_agg = {\n",
    "    'svd__n_components': [50, 100, 200, 300],\n",
    "    'cluster__n_clusters': list(range(2, 30))\n",
    "}\n",
    "grid_agg = GridSearchCV(\n",
    "    pipe_agg,\n",
    "    param_grid_agg,\n",
    "    scoring=silhouette_scorer,\n",
    "    cv=[(\n",
    "        np.arange(X_tfidf_dense.shape[0]),\n",
    "        np.arange(X_tfidf_dense.shape[0])\n",
    "    )],\n",
    "    return_train_score=False\n",
    ")\n",
    "grid_agg.fit(X_tfidf_dense)\n",
    "best_params_agg = grid_agg.best_params_\n",
    "# Best Agglomerative pipeline params: {'cluster__n_clusters': 16, 'svd__n_components': 50}\n",
    "print(f\"Best Agglomerative pipeline params: {best_params_agg}\")\n",
    "\n",
    "df['Cluster_Agg_Pipe'] = grid_agg.best_estimator_.named_steps['cluster'].labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed526a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_km_pipe = silhouette_score(X_tfidf_dense, df['Cluster_KMeans_Pipe'])\n",
    "score_agg_pipe = silhouette_score(X_tfidf_dense, df['Cluster_Agg_Pipe'])\n",
    "print(f\"Pipeline KMeans Silhouette = {score_km_pipe:.4f}\")\n",
    "print(f\"Pipeline Agglomerative Silhouette = {score_agg_pipe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b02e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vis = PCA(n_components=2).fit_transform(X_tfidf_dense)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes[0].scatter(X_vis[:,0], X_vis[:,1], c=df['Cluster_KMeans_Pipe'], cmap='rainbow', alpha=0.6)\n",
    "axes[0].set_title(f\"KMeans Pipeline (n_clusters={best_params_km['cluster__n_clusters']}, n_pca={best_params_km['pca__n_components']})\")\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "axes[1].scatter(X_vis[:,0], X_vis[:,1], c=df['Cluster_Agg_Pipe'], cmap='rainbow', alpha=0.6)\n",
    "axes[1].set_title(f\"Agglomerative Pipeline (n_clusters={best_params_agg['cluster__n_clusters']}, n_pca={best_params_agg['pca__n_components']})\")\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c4143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55341b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_kmeans(X_tune, cluster_range_tunning, random_state=random_state):\n",
    "    best_score = -1\n",
    "    best_k = 0\n",
    "    scores = {}\n",
    "    for k in cluster_range_tunning:\n",
    "        km = KMeans(n_clusters=k, random_state=random_state)\n",
    "        labels = km.fit_predict(X_tune)\n",
    "        score = silhouette_score(X_tune, labels)\n",
    "        scores[k] = score\n",
    "        print(f\"K={k}: Silhouette Score = {score}\")\n",
    "        if score > best_score:\n",
    "            best_score, best_k = score, k\n",
    "    print(f\"Best K: {best_k} with Silhouette Score = {best_score:.4f}\")\n",
    "    return best_k, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200674e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range_km = range(19, 25)  # Testing k from 19 to 24\n",
    "best_k_km, k_scores_km = tune_kmeans(X_tfidf, k_range_km, random_state)\n",
    "print(f\"Optimal number of clusters (k): {best_k_km}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range_km, k_scores_km.values(), marker='o')\n",
    "plt.title('Silhouette Scores for Different k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a045c7",
   "metadata": {},
   "source": [
    "Based on the plots, select the optimal k. For demonstration, if elbow is at 5 and silhouette peaks around there, we stick with 5.\n",
    "If a different k looks better (e.g., higher silhouette), update accordingly. Here, we assume 5 is still reasonable but note any observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_opt = KMeans(n_clusters=best_k_km, random_state=random_state)\n",
    "df['Cluster_TFIDF_Opt'] = kmeans_opt.fit_predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ca403",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_dense = X_tfidf.toarray()\n",
    "X_tfidf_pca = PCA(n_components=2).fit_transform(X_tfidf_dense)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_tfidf_pca[:,0], X_tfidf_pca[:,1], c=df['Cluster_TFIDF_Opt'], cmap='rainbow', alpha=0.6)\n",
    "plt.title(\"PCA of TF-IDF Clusters\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5790f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE \n",
    "\n",
    "# Run t-SNE on TFIDF vectors\n",
    "tsne_tfidf = TSNE(n_components=2, random_state=random_state, perplexity=30, max_iter=1000, init='random')\n",
    "X_tsne_tfidf = tsne_tfidf.fit_transform(X_tfidf)\n",
    "\n",
    "# Plot TFIDF t-SNE\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.scatter(X_tsne_tfidf[:,0], X_tsne_tfidf[:,1], c=df['Cluster_TFIDF_Opt'], cmap='tab10', alpha=0.6)\n",
    "plt.title(\"t-SNE of Clusters (TFIDF vectors)\")\n",
    "plt.xlabel(\"t-SNE dim 1\")\n",
    "plt.ylabel(\"t-SNE dim 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only TFIDF UMAP\n",
    "umap_tfidf = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=random_state)\n",
    "X_umap_tfidf = umap_tfidf.fit_transform(X_tfidf)\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.scatter(X_umap_tfidf[:,0], X_umap_tfidf[:,1], c=df['Cluster_TFIDF_Opt'], cmap='tab10', alpha=0.6)\n",
    "plt.title(\"UMAP of Clusters (TFIDF vectors)\")\n",
    "plt.xlabel(\"UMAP dim 1\")\n",
    "plt.ylabel(\"UMAP dim 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d839d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(original_data, method='ward', metric='euclidean', subset_size=1000):\n",
    "    \n",
    "    subset_size = 1000  # Adjust based on computational resources\n",
    "    X_tfidf_subset = original_data[:subset_size].toarray()  # Convert to dense for linkage\n",
    "\n",
    "    # Compute linkage matrix for dendrogram\n",
    "    Z = linkage(X_tfidf_subset, method=method, metric=metric)\n",
    "\n",
    "    # Plot Dendrogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(Z, truncate_mode='level', p=5)  # Truncate for readability\n",
    "    plt.title('Dendrogram for Hierarchical Clustering (Subset)')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance (Ward)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(X_tfidf, method='ward', metric='euclidean', subset_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_agglomerative(X_tune, cluster_range_tunning, random_state=random_state):\n",
    "    best_score = -1\n",
    "    best_k_ = None\n",
    "    scores_ = {}\n",
    "    for k in cluster_range_tunning:\n",
    "        agg_cluster = AgglomerativeClustering(n_clusters=k)\n",
    "        labels = agg_cluster.fit_predict(X_tune)\n",
    "        score = silhouette_score(X_tune, labels)\n",
    "        scores_[k] = score\n",
    "        print(f\"K={k}: Silhouette Score = {score}\")\n",
    "        if score > best_score:\n",
    "            best_score, best_k_ = score, k\n",
    "    print(f\"Best K: {best_k_} with Silhouette Score = {best_score:.4f}\")\n",
    "    return best_k_, scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range_agg = range(2, 6)  # Testing k from 2 to 5\n",
    "best_k_agg, k_scores_agg = tune_agglomerative(X_tfidf.toarray(), k_range_agg, random_state)\n",
    "print(f\"Optimal number of agglomerative clusters (k): {best_k_agg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range_agg, k_scores_agg.values(), marker='o')\n",
    "plt.title('Silhouette Scores for Different k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cluster = AgglomerativeClustering(n_clusters=best_k_agg, linkage='ward', metric='euclidean')\n",
    "df['Cluster_Hierarchical'] = agg_cluster.fit_predict(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_hier = PCA(n_components=2).fit_transform(X_tfidf.toarray())\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca_hier[:,0], X_pca_hier[:,1], c=df['Cluster_Hierarchical'], cmap='rainbow', alpha=0.6)\n",
    "plt.title(\"PCA of Hierarchical Clusters\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_hier = TSNE(n_components=2, random_state=random_state, perplexity=30, max_iter=1000, init='random')\n",
    "X_tsne_hier = tsne_hier.fit_transform(X_tfidf)\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.scatter(X_tsne_hier[:,0], X_tsne_hier[:,1], c=df['Cluster_Hierarchical'], cmap='tab10', alpha=0.6)\n",
    "plt.title(\"t-SNE of Hierarchical Clusters (TFIDF vectors)\")\n",
    "plt.xlabel(\"t-SNE dim 1\")\n",
    "plt.ylabel(\"t-SNE dim 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93796ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_hier = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=random_state)\n",
    "X_umap_hier = umap_hier.fit_transform(X_tfidf)\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.scatter(X_umap_hier[:,0], X_umap_hier[:,1], c=df['Cluster_Hierarchical'], cmap='tab10', alpha=0.6)\n",
    "plt.title(\"UMAP of Hierarchical Clusters (TFIDF vectors)\")\n",
    "plt.xlabel(\"UMAP dim 1\")\n",
    "plt.ylabel(\"UMAP dim 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299823cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(best_k_km):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"KMeans Cluster: {top_tfidf_words_per_cluster(X_tfidf, df['Cluster_TFIDF_Opt'], vectorizer, i)}\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "for i in range(best_k_agg):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"nHierarchical Cluster: {top_tfidf_words_per_cluster(X_tfidf, df['Cluster_Hierarchical'], vectorizer, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, adjusted_rand_score\n",
    "\n",
    "sil_kmeans = silhouette_score(X_tfidf, df['Cluster_TFIDF_Opt'])\n",
    "db_kmeans = davies_bouldin_score(X_tfidf.toarray(), df['Cluster_TFIDF_Opt'])\n",
    "sil_hier = silhouette_score(X_tfidf, df['Cluster_Hierarchical'])\n",
    "db_hier = davies_bouldin_score(X_tfidf.toarray(), df['Cluster_Hierarchical'])\n",
    "\n",
    "ari = adjusted_rand_score(df['Cluster_TFIDF_Opt'], df['Cluster_Hierarchical'])\n",
    "\n",
    "print(f\"KMeans Silhouette Score: {sil_kmeans:.4f}\")\n",
    "print(f\"Hierarchical Silhouette Score: {sil_hier:.4f}\")\n",
    "print(f\"KMeans Davies-Bouldin Index: {db_kmeans:.4f}\")\n",
    "print(f\"Hierarchical Davies-Bouldin Index: {db_hier:.4f}\")\n",
    "print(f\"Adjusted Rand Index between KMeans and Hierarchical: {ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4ffc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csca-5632-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
